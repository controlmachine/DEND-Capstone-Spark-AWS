name: Push Docker Image to ECR , Upload to S3 and Deploy CloudFormation stack

on:
  push:
    branches:
      - main

jobs:
  docker:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2
    - name: Set up AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    - name: Dockerize data_quality_checks file and push to ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        cd data_quality_checks
        docker build -t data-quality-ecr:latest .
        docker tag data-quality-ecr:latest $ECR_REGISTRY/data-quality-ecr:latest
        docker push $ECR_REGISTRY/data-quality-ecr:latest
        cd ..
    outputs:
      ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}

  s3:
    runs-on: ubuntu-latest
    needs: docker
    if: success() && needs.docker.result == 'success'
    steps:
    - name: Checkout code
      uses: actions/checkout@v2
    - name: Set up AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2
          
    - name: Set up Python 3.7
      uses: actions/setup-python@v1
      with:
        python-version: 3.7

    - name: Install AWS CLI
      run: |
        pip install awscli --upgrade --user
        export PATH=$PATH:$HOME/.local/bin
        
    - name: Upload spark-app.py to S3
      run: |
        docker run --rm \
          -v $PWD/spark-app:/app \
          amazon/aws-cli s3 cp /app/main.py s3://mysparkawsbucket/main.py
        docker run --rm \
          -v $PWD/spark-app:/app \
          amazon/aws-cli s3 cp /app/file_processor.py s3://mysparkawsbucket/file_processor.py
        docker run --rm \
          -v $PWD/spark-app:/app \
          amazon/aws-cli s3 cp /app/function_lookup.py s3://mysparkawsbucket/function_lookup.py
        docker run --rm \
          -v $PWD/spark-app:/app \
          amazon/aws-cli s3 cp /app/utils.py s3://mysparkawsbucket/utils.py

  cloudformation:
    runs-on: ubuntu-latest
    needs: docker
    if: success() && needs.docker.result == 'success'
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      - name: Set up AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2
      - name: Deploy CloudFormation stack
        uses: aws-actions/aws-cloudformation-github-deploy@master
        with:
          template: cloudformation/cloudformation.yml
          name: my-cloudformation-stack
          capabilities: CAPABILITY_NAMED_IAM
          parameter-overrides: "CatalogId=${{ secrets.CATALOG_ID }},ImageUri=${{ secrets.IMAGE_URI }}"
