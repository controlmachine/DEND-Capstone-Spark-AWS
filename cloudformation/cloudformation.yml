AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  CatalogId:
    Type: String
    Description: The Catalog ID for the Glue table.
    Default: '1234567'
  ImageUri:
      Type: String
      Description: ImageURI for lambda.
      Default: '1234567'
Resources:
  AthenaDatabase:
    Type: 'AWS::Glue::Database'
    Properties:
      CatalogId: !Ref CatalogId
      DatabaseInput:
        Name: mydatabase
        Description: My Athena database
        
  AthenaTable1:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: vendor
        StorageDescriptor:
          Columns:
            - Name: vendor_number
              Type: int
              Comment: The number of the vendor
            - Name: vendor_name
              Type: string
              Comment: The name of the vendor
          Location: s3://sparkcapstonebucket/test_key/vendor_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'

  AthenaTable2:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: store
        StorageDescriptor:
          Columns:
            - Name: store_number
              Type: int
              Comment: The unique identifier of the store
            - Name: store_name
              Type: string
              Comment: The name of the store
            - Name: address
              Type: string
              Comment: The address of the store
            - Name: store_location
              Type: string
              Comment: The location of the store
          Location: s3://sparkcapstonebucket/test_key/store_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'

  AthenaTable3:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: item
        StorageDescriptor:
          Columns:
            - Name: item_number
              Type: int
              Comment: The unique identifier of the item
            - Name: item_description
              Type: string
              Comment: The description of the item
          Location: s3://sparkcapstonebucket/test_key/item_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'

  AthenaTable4:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: city
        StorageDescriptor:
          Columns:
            - Name: zip_code
              Type: string
              Comment: The unique identifier of the city
            - Name: city
              Type: string
              Comment: The city name
          Location: s3://sparkcapstonebucket/test_key/city_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'


  AthenaTable5:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: category
        StorageDescriptor:
          Columns:
            - Name: category
              Type: int
              Comment: The unique identifier of the category
            - Name: category_name
              Type: string
              Comment: The category name
          Location: s3://sparkcapstonebucket/test_key/category_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'

  AthenaTable6:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: time
        StorageDescriptor:
          Columns:
            - Name: date
              Type: string
              Comment: The unique identifier of the date
            - Name: date_ex
              Type: date
              Comment: The unique identifier of the date
            - Name: weekend
              Type: int
              Comment: Weekend or not
            - Name: year
              Type: int
              Comment: Year of the date
            - Name: month
              Type: int
              Comment: Month of the date
            - Name: quarter
              Type: int
              Comment: Quarter of the date
          Location: s3://sparkcapstonebucket/test_key/time_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'

  AthenaTable7:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: item_price
        StorageDescriptor:
          Columns:
            - Name: item_number
              Type: int
              Comment: The unique identifier of the item
            - Name: date
              Type: string
              Comment: The description of the item
            - Name: state_bottle_cost_dollar
              Type: double
              Comment: Cost of a bottle item
            - Name: state_bottle_retail_dollar
              Type: double
              Comment: Retail of the bottle item
          Location: s3://sparkcapstonebucket/test_key/item_price_table.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'   

  AthenaTable8:
    Type: 'AWS::Glue::Table'
    Properties:
      DatabaseName: !Ref AthenaDatabase
      CatalogId: !Ref CatalogId
      TableInput:
        Name: orders
        StorageDescriptor:
          Columns:
            - Name: invoice_number
              Type: string
              Comment: The unique identifier of the invoice
            - Name: date
              Type: string
              Comment: The date of the item
            - Name: store_number
              Type: int
              Comment: Store Number
            - Name: zip_code
              Type: string
              Comment: Zip code of City
            - Name: country_number
              Type: string
              Comment: The unique identifier of the country
            - Name: vendor_number
              Type: int
              Comment: Vendor Number
            - Name: item_number
              Type: int
              Comment: Identifier of the item
            - Name: category_number
              Type: int
              Comment: Category of the item
            - Name: bottles_sold
              Type: int
              Comment: How many bottle sold in one transaction
            - Name: volume_sold_liters
              Type: float
              Comment: How much volume sold in ml
            - Name: sale_dollar
              Type: double
              Comment: How much a sale total
          Location: s3://sparkcapstonebucket/test_key/order_fact.parquet/
          InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
          Parameters:
            'compression': 'SNAPPY'  

  EMRCluster:
    Type: AWS::EMR::Cluster
    Properties:
      Instances:
        CoreInstanceGroup:
          InstanceType: "m5.xlarge"
          InstanceCount: 1
        MasterInstanceGroup:
          InstanceType: "m5.xlarge"
          InstanceCount: 1
        TaskInstanceGroups:
         - Name: "Task1"
           InstanceType: "m5.xlarge"
           InstanceCount: 1
      JobFlowRole: "arn:aws:iam::740062030037:instance-profile/EMR_EC2_DefaultRole"
      ServiceRole: "arn:aws:iam::740062030037:role/EMR_DefaultRole"
      Name: "MyEMRCluster"
      ReleaseLabel: "emr-6.2.0"
      Applications:
        - Name: "Spark"
      Configurations:
        - Classification: capacity-scheduler
          ConfigurationProperties:
            yarn.scheduler.capacity.root.default.maximum-capacity: 100
            yarn.scheduler.capacity.root.default.capacity: 100
      ManagedScalingPolicy:
        ComputeLimits:
          MaximumCapacityUnits: 20
          MinimumCapacityUnits: 2
          MaximumCoreCapacityUnits: 20
          MaximumOnDemandCapacityUnits: 20
          UnitType: Instances
      VisibleToAllUsers: true
      LogUri : s3://mysparkawsbucket/
      BootstrapActions: []
      Steps:
        - Name: "Run custom script"
          ActionOnFailure: CONTINUE
          HadoopJarStep:
            Jar: "command-runner.jar"
            Args:
              - "spark-submit"
              - "s3://mysparkawsbucket/spark-app.py"


  MyLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      PackageType: Image
      Code:
        ImageUri:  !Ref ImageUri
      Role: !GetAtt MyLambdaExecutionRole.Arn
      Timeout: 300

  MyLambdaExecutionPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: MyLambdaExecutionPolicy
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - ecr:GetAuthorizationToken
              - ecr:BatchCheckLayerAvailability
              - ecr:GetDownloadUrlForLayer
              - ecr:BatchGetImage
            Resource: "*"
      Roles:
        - !Ref MyLambdaExecutionRole
  MyLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: MyLambdaPermissions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:*
                Resource: "*"
              - Effect: Allow
                Action:
                  - glue:*
                Resource: "*"
              - Effect: Allow
                Action:
                  - athena:*
                Resource: "*"